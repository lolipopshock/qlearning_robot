{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 Algorithm Understanding \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introduction to Reinforcement Learning\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 Mechanisms and Core Components\n",
    "\n",
    "There are five core components in a reinforcement learning problem, i.e, **Environment**, **Agent**, **State**, **Action**, and **Reward**. Agent perceive which state is it from the environment and choose actions based on the state. For each action, agent receives a reward from the environment and will move to next state according to the given action. Combined with appropriate algorithms, our agent will be able to learn something from such setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Question 1: Please describe the corresponding elementes in this maze problem. \n",
    "    - **Environment**: \n",
    "    - **Agent**: \n",
    "    - **State**: \n",
    "    - **Action**: \n",
    "    - **Reward**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Markov Decision Process \n",
    "\n",
    "A Markov Decision Process is made up of 5 parts, namely, a finite set of states, a series of rewards, a transition matrix, a finite set of actions, and a discount factor.\n",
    "\n",
    "Markov property: the future state is only dependent on current state rather than whole history ahead. $P(S_{t+1}=s \\mid S_t) = P(S_{t+1}=s \\mid S_t, S_{t-1}, ..., S_1)$\n",
    "\n",
    "Reward of a action at given state $R_a(s)$: The reward received from the environment when choosing action $a$ at state $s$.\n",
    "\n",
    "Transition Matrix $\\mathcal{P}$: At state $s$ and for a given action $a$, reaching state $s’$ is not always deterministic. Hence, a transition matrix $\\mathcal{P}_{ss’}^a$ is used to describe such condition. $\\mathcal{P}_{ss’}^a=P(S_{t+1}=s'\\mid S_{t}=s,A_t=a)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Question 2.1: Given action $a$ and transition matrix $\\mathcal{P}$, What's the probability of robot arriving at state $s_1$ from state $s_0$? Assume that the robot always choosing the action with highest policy value. \n",
    "\n",
    "Question 2.2: What is the expected reward of the robot when choosing action $a$ at state $s$ according to the information given above? Reminder: How to calculate the expect of a random variable: $ \\mathbb{E}[X] = \\sum x_ip_i$.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How to choose action \n",
    "\n",
    "Policy $\\pi$: A mapping from state to action. Describing how to choose action at a given state. $\\pi(s)=P(A=a\\mid S=s)$. Generally, we will choose the action will the highest return in this scenario. \n",
    "\n",
    "Please note that highest return does not mean the highest current reward of a action. Just like what humans do when choosing a action, an agent will sometimes consider the long term reward instead of being short-sighted. \n",
    "\n",
    "A value `v` is an evaluation of long term reward of a action. $v_{\\pi}(s)$ describes the expected value of a given policy $\\pi$ and $q(a,s)$ for the value of a action at some state $s$. \n",
    "\n",
    "$$\n",
    "q(a,s) = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ... =  R_t +\\sum_{k=1}^\\infty  \\gamma^k R_{t+k}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
